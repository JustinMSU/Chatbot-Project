{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c2d93b-0a64-4500-94f4-ccdc746385c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.nn import init\n",
    "import torch.nn.utils.rnn \n",
    "import datetime\n",
    "import operator\n",
    "import codecs\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f0275b-2252-40d2-94ae-acb14e350427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(csvfile):\n",
    "    dataframe = pd.read_csv(csvfile)\n",
    "    return dataframe\n",
    "\n",
    "def shuffle_dataframe(dataframe):\n",
    "    dataframe.reindex(np.random.permutation(dataframe.index))\n",
    "\n",
    "def create_vocab(dataframe):\n",
    "    vocab = []\n",
    "    word_freq = {}\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        context_cell = row[\"Context\"]\n",
    "        response_cell = row[\"Utterance\"]\n",
    "        \n",
    "        train_words = str(context_cell).split() + str(response_cell).split()\n",
    "        \n",
    "        for word in train_words:\n",
    "          \n",
    "            if word.lower() not in vocab:\n",
    "                vocab.append(word.lower())         \n",
    "                       \n",
    "            if word.lower() not in word_freq:\n",
    "                word_freq[word.lower()] = 1\n",
    "            else:\n",
    "                word_freq[word] += 1\n",
    "    \n",
    "    word_freq_sorted = sorted(word_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "    vocab = [\"<UNK>\"] + [pair[0] for pair in word_freq_sorted]\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def create_word_to_id(vocab):             \n",
    "    word_to_id = {word: id for id, word in enumerate(vocab)}\n",
    "    \n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def create_id_to_vec(word_to_id, glovefile): \n",
    "    lines = open(glovefile, 'r', encoding='utf-8').readlines()\n",
    "    id_to_vec = {}\n",
    "    vector = None\n",
    "    \n",
    "    for line in lines:\n",
    "        word = line.split()[0]\n",
    "        vector = np.array(line.split()[1:], dtype='float32') #32\n",
    "        \n",
    "        if word in word_to_id:\n",
    "            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(vector))\n",
    "            \n",
    "    for word, id in word_to_id.items(): \n",
    "        if word_to_id[word] not in id_to_vec:\n",
    "            v = np.zeros(*vector.shape, dtype='float32')\n",
    "            v[:] = np.random.randn(*v.shape)*0.01\n",
    "            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(v))\n",
    "            \n",
    "    embedding_dim = id_to_vec[0].shape[0]\n",
    "    \n",
    "    return id_to_vec, embedding_dim\n",
    "\n",
    "\n",
    "def load_ids_and_labels(row, word_to_id):\n",
    "    context_ids = []\n",
    "    response_ids = []\n",
    "\n",
    "    context_cell = row['Context']\n",
    "    response_cell = row['Utterance']\n",
    "    label_cell = row['Label']\n",
    "\n",
    "    max_context_len = 160\n",
    "    \n",
    "    context_words = context_cell.split()\n",
    "    if len(context_words) > max_context_len:\n",
    "        context_words = context_words[:max_context_len]\n",
    "    for word in context_words:\n",
    "        if word in word_to_id:\n",
    "            context_ids.append(word_to_id[word])\n",
    "        else: \n",
    "            context_ids.append(0) #UNK\n",
    "    \n",
    "    response_words = response_cell.split()\n",
    "    for word in response_words:\n",
    "        if word in word_to_id:\n",
    "            response_ids.append(word_to_id[word])\n",
    "        else: \n",
    "            response_ids.append(0)\n",
    "    \n",
    "    label = np.array(label_cell).astype(np.float32)\n",
    "\n",
    "    return context_ids, response_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de4bb6a-228e-4fd1-9d01-fb3357822fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "            emb_size, \n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            p_dropout): \n",
    "    \n",
    "            super(Encoder, self).__init__()\n",
    "             \n",
    "            self.emb_size = emb_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.vocab_size = vocab_size\n",
    "            self.p_dropout = p_dropout\n",
    "       \n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "            self.lstm = nn.LSTM(self.emb_size, self.hidden_size)\n",
    "            self.dropout_layer = nn.Dropout(self.p_dropout) \n",
    "\n",
    "            self.init_weights()\n",
    "             \n",
    "    def init_weights(self):\n",
    "        init.uniform(self.lstm.weight_ih_l0, a = -0.01, b = 0.01)\n",
    "        init.orthogonal(self.lstm.weight_hh_l0)\n",
    "        self.lstm.weight_ih_l0.requires_grad = True\n",
    "        self.lstm.weight_hh_l0.requires_grad = True\n",
    "        \n",
    "        embedding_weights = torch.FloatTensor(self.vocab_size, self.emb_size)\n",
    "            \n",
    "        for id, vec in id_to_vec.items():\n",
    "            embedding_weights[id] = vec\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(embedding_weights, requires_grad = True)\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        _, (last_hidden, _) = self.lstm(embeddings) #dimensions: (num_layers * num_directions x batch_size x hidden_size)\n",
    "        last_hidden = self.dropout_layer(last_hidden[-1])#access last lstm layer, dimensions: (batch_size x hidden_size)\n",
    "\n",
    "        return last_hidden\n",
    "\n",
    "    \n",
    "class DualEncoder(nn.Module):\n",
    "     \n",
    "    def __init__(self, encoder):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.hidden_size = self.encoder.hidden_size\n",
    "        M = torch.FloatTensor(self.hidden_size, self.hidden_size)     \n",
    "        init.xavier_normal(M)\n",
    "        self.M = nn.Parameter(M, requires_grad = True)\n",
    "\n",
    "    def forward(self, context_tensor, response_tensor):\n",
    "        \n",
    "        context_last_hidden = self.encoder(context_tensor) #dimensions: (batch_size x hidden_size)\n",
    "        response_last_hidden = self.encoder(response_tensor) #dimensions: (batch_size x hidden_size)\n",
    "        \n",
    "        #context = context_last_hidden.mm(self.M).cuda()\n",
    "        context = context_last_hidden.mm(self.M) #dimensions: (batch_size x hidden_size)\n",
    "        context = context.view(-1, 1, self.hidden_size) #dimensions: (batch_size x 1 x hidden_size)\n",
    "        \n",
    "        response = response_last_hidden.view(-1, self.hidden_size, 1) #dimensions: (batch_size x hidden_size x 1)\n",
    "        \n",
    "        #score = torch.bmm(context, response).view(-1, 1).cuda()\n",
    "        score = torch.bmm(context, response).view(-1, 1) #dimensions: (batch_size x 1 x 1) and lastly --> (batch_size x 1)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dad5ed7-b1b2-436f-8427-0db67a277194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_variables(num_training_examples, num_validation_examples, embedding_dim):\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Creating variables for training and validation...\")\n",
    "\n",
    "    training_dataframe = create_dataframe('dataset/MAIN FILES/training_%d.csv' %num_training_examples)\n",
    "    vocab = create_vocab(training_dataframe)\n",
    "    word_to_id = create_word_to_id(vocab)\n",
    "    id_to_vec, emb_dim = create_id_to_vec(word_to_id, 'dataset/GloVe/glove.6B.%dd.txt' %embedding_dim)\n",
    "\n",
    "    validation_dataframe = create_dataframe('dataset/MAIN FILES/validation_%d.csv' %num_validation_examples)\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Variables created.\\n\")\n",
    "    \n",
    "    return training_dataframe, vocab, word_to_id, id_to_vec, emb_dim, validation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e57e10-cf8d-44b5-8041-059ff9b527aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_model(hidden_size, p_dropout):\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Calling model...\")\n",
    "\n",
    "    encoder = Encoder(\n",
    "            emb_size = emb_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            vocab_size = len(vocab),\n",
    "            p_dropout = p_dropout)\n",
    "\n",
    "    dual_encoder = DualEncoder(encoder)\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Model created.\\n\")\n",
    "    print(dual_encoder)\n",
    "    \n",
    "    return encoder, dual_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422ded90-db62-40ed-a3c9-a671def3189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_count(correct_count, score, label):\n",
    "    if ((score.data[0][0] >= 0.5) and (label.data[0][0] == 1.0)) or ((score.data[0][0] < 0.5) and (label.data[0][0]  == 0.0)):\n",
    "       correct_count +=1  \n",
    "   \n",
    "    return correct_count\n",
    "\n",
    "def get_accuracy(correct_count, dataframe):\n",
    "    accuracy = correct_count/(len(dataframe))\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e06d796-7b7e-407a-9b6e-5eb14fa27820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, l2_penalty, epochs): \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Starting training and validation...\\n\")\n",
    "    print(\"====================Data and Hyperparameter Overview====================\\n\")\n",
    "    print(\"Number of training examples: %d, Number of validation examples: %d\" %(len(training_dataframe), len(validation_dataframe)))\n",
    "    print(\"Learning rate: %.5f, Embedding Dimension: %d, Hidden Size: %d, Dropout: %.2f, L2:%.10f\\n\" %(learning_rate, emb_dim, encoder.hidden_size, encoder.p_dropout, l2_penalty))\n",
    "    print(\"================================Results...==============================\\n\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(dual_encoder.parameters(), lr = learning_rate, weight_decay = l2_penalty)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    #loss_func.cuda()\n",
    "    best_validation_accuracy = 0.0\n",
    "     \n",
    "    for epoch in range(epochs): \n",
    "                     \n",
    "            shuffle_dataframe(training_dataframe)\n",
    "            sum_loss_training = 0.0\n",
    "            training_correct_count = 0\n",
    "            dual_encoder.train()\n",
    "\n",
    "            for index, row in training_dataframe.iterrows():            \n",
    "            \n",
    "                context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n",
    "                context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1), requires_grad = False)#.cuda()\n",
    "                response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1), requires_grad = False)#.cuda()\n",
    "                label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1))), requires_grad = False)#.cuda()\n",
    "                             \n",
    "                score = dual_encoder(context, response)\n",
    "                loss = loss_func(score, label)\n",
    "                sum_loss_training += loss.item() #loss.data[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                training_correct_count = increase_count(training_correct_count, score, label)\n",
    "                                                    \n",
    "            training_accuracy = get_accuracy(training_correct_count, training_dataframe)\n",
    "            #plt.plot(epoch, training_accuracy)\n",
    "                \n",
    "            shuffle_dataframe(validation_dataframe)\n",
    "            \n",
    "            validation_correct_count = 0\n",
    "            sum_loss_validation = 0.0\n",
    "            dual_encoder.eval()\n",
    "\n",
    "            for index, row in validation_dataframe.iterrows():\n",
    "                \n",
    "                context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n",
    "                context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1)) #.cuda()\n",
    "                response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1)) #.cuda()\n",
    "                label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1)))) #.cuda()\n",
    "                \n",
    "                score = dual_encoder(context, response)\n",
    "                loss = loss_func(score, label)\n",
    "                sum_loss_validation += loss.item() #loss.data[0]\n",
    "                validation_correct_count = increase_count(validation_correct_count, score, label)\n",
    "                    \n",
    "            validation_accuracy = get_accuracy(validation_correct_count, validation_dataframe)\n",
    "                        \n",
    "            print(str(datetime.datetime.now()).split('.')[0], \n",
    "                  \"Epoch: %d/%d\" %(epoch,epochs),  \n",
    "                  \"TrainLoss: %.3f\" %(sum_loss_training/len(training_dataframe)), \n",
    "                  \"TrainAccuracy: %.3f\" %(training_accuracy), \n",
    "                  \"ValLoss: %.3f\" %(sum_loss_validation/len(validation_dataframe)), \n",
    "                  \"ValAccuracy: %.3f\" %(validation_accuracy))\n",
    "            \n",
    "            if validation_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                torch.save(dual_encoder.state_dict(), 'dataset/models/saved_model_%d_examples.pt' %(len(training_dataframe)))\n",
    "                print(\"New best found and saved.\")\n",
    "                \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Training and validation epochs finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee78e836-1fb5-4b6e-863a-69d82189e143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-07 13:35:07 Creating variables for training and validation...\n",
      "2021-11-07 13:35:30 Variables created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataframe, vocab, word_to_id, id_to_vec, emb_dim, validation_dataframe = creating_variables(num_training_examples = 10000, \n",
    "                                                                                                     embedding_dim = 50, \n",
    "                                                                                                     num_validation_examples = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c2f8a33-e7b6-4a71-8be9-c148141ced82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-07 13:35:30 Calling model...\n",
      "2021-11-07 13:35:30 Model created.\n",
      "\n",
      "DualEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(29997, 50)\n",
      "    (lstm): LSTM(50, 50)\n",
      "    (dropout_layer): Dropout(p=0.85, inplace=False)\n",
      "  )\n",
      ")\n",
      "M\n",
      "encoder.embedding.weight\n",
      "encoder.lstm.weight_ih_l0\n",
      "encoder.lstm.weight_hh_l0\n",
      "encoder.lstm.bias_ih_l0\n",
      "encoder.lstm.bias_hh_l0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-53be591e4b9f>:23: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  init.uniform(self.lstm.weight_ih_l0, a = -0.01, b = 0.01)\n",
      "<ipython-input-4-53be591e4b9f>:24: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  init.orthogonal(self.lstm.weight_hh_l0)\n",
      "<ipython-input-4-53be591e4b9f>:50: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(M)\n"
     ]
    }
   ],
   "source": [
    "encoder, dual_encoder = creating_model(hidden_size = 50, \n",
    "                                       p_dropout = 0.85)\n",
    "\n",
    "#encoder.cuda()\n",
    "#dual_encoder.cuda\n",
    "\n",
    "for name, param in dual_encoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a965cc0-927a-4dd9-92c3-a303f01cabb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-07 14:30:07 Starting training and validation...\n",
      "\n",
      "====================Data and Hyperparameter Overview====================\n",
      "\n",
      "Number of training examples: 10000, Number of validation examples: 1000\n",
      "Learning rate: 0.00010, Embedding Dimension: 50, Hidden Size: 50, Dropout: 0.85, L2:0.0001000000\n",
      "\n",
      "================================Results...==============================\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-fb3014ba3ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_model(learning_rate = 0.0001, \n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0ml2_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             epochs = 100)\n",
      "\u001b[1;32m<ipython-input-27-59182c7e32a2>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(learning_rate, l2_penalty, epochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdual_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0msum_loss_validation\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \u001b[0mvalidation_correct_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mincrease_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_correct_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "train_model(learning_rate = 0.0001, \n",
    "            l2_penalty = 0.0001,\n",
    "            epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c6ba2-1418-4512-99c7-a4dc8471fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dual_encoder.load_state_dict(torch.load('dataset/models/saved_model_10000_examples.pt'))\n",
    "\n",
    "dual_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2de34-b6f6-4764-9277-a0cc494ae52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe_same_structure = pd.read_csv('testing_same_structure_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ccc47-15fc-4c37-be61-50091866602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_same_structure():\n",
    "    \n",
    "    test_correct_count = 0\n",
    "\n",
    "    for index, row in test_dataframe_same_structure.iterrows():\n",
    "\n",
    "        context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n",
    "        context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1)) #.cuda()\n",
    "        response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1)) #.cuda()\n",
    "        label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1)))) #.cuda()\n",
    "\n",
    "        score = dual_encoder(context, response)\n",
    "        test_correct_count = increase_count(test_correct_count, score, label)\n",
    "\n",
    "    test_accuracy = get_accuracy(test_correct_count, test_dataframe_same_structure)\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da6288-76da-43af-aa9f-47c77f75991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = testing_same_structure()\n",
    "print(\"Test accuracy for %d training examples and %d test examples: %.2f\" %(len(training_dataframe),len(test_dataframe_same_structure),test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
